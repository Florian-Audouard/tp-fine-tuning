{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dbc018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: loralib in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: bert_score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (3.2.5)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\n",
      "Requirement already satisfied: bitsandbytes==0.45.3 in /usr/local/lib/python3.12/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.45.3) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.45.3) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes==0.45.3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.3) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch \\\n",
    "    torchdata --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    evaluate \\\n",
    "    rouge_score \\\n",
    "    loralib \\\n",
    "    peft \\\n",
    "    bert_score\n",
    "%pip install bitsandbytes==0.45.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfb978b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "import time\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import bitsandbytes\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a0893",
   "metadata": {},
   "source": [
    "## Fine tuning de Llama 3 ou Qwen 3 1.7B\n",
    "\n",
    "Le modèle `flan-t5-base`que nous avons utilisé jusqu'à maintenant est bien pour comprendre les principes mais c'est un modèle ancien aux performances dépassées par rapport aux modèles récents tels que Llama 3.\n",
    "\n",
    "Dans cet exercice, vous allez charger puis fine tuner un LLM bien plus performant tout en conservant une taille acceptable de 3B de paramètres : Llama 3.2 - 3B. Nous pouvons aussi tester avec Qwen 3 1.7B (https://huggingface.co/Qwen/Qwen3-1.7B).\n",
    "\n",
    "Afin que le modèle puisse être chargé en VRAM, nous utiliserons une version quantisée en 4bits : https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-bnb-4bit. L'utilisation de la bibliothèque `bitsandbytes`est alors indispensable.\n",
    "\n",
    "**Redémarrer la session à ce stade pour réinitialiser la RAM et la VRAM**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba8c02",
   "metadata": {},
   "source": [
    "### Conseils pour réaliser l'exercice :\n",
    "\n",
    "-   Le modèle n'est plus de type _Encoder Decoder_ (Seq2Seq) mais _Decoder only_ (CausalLM). Effectuer les modifications en conséquence\n",
    "-   Réduire la taille du jeu de données d'entraînement pour rester dans des temps acceptables (100 exemples)\n",
    "-   Modifier les arguments d'entraînement (`TrainingArguments`) pour prendre accélérer le traitement : considérer les paramètres `per_device_train_batch_size`, `gradient_accumulation_steps`, `gradient_chekpointing`.\n",
    "\n",
    "L'exercice peut prendre un certain temps, faites votre maximum et avancer pas à pas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5d8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LLaMA models don't have a pad token - set it to eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Left padding is better for causal LM generation\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bae335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name).filter(\n",
    "    lambda x, index: index % 100 == 0, with_indices=True\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example[\"input_ids\"] = tokenizer(\n",
    "        prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    example[\"labels\"] = tokenizer(\n",
    "        example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"topic\",\n",
    "        \"dialogue\",\n",
    "        \"summary\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2aefb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Vérifier le nombre de paramètres entraînables après LoRA plus bas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da78137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d97b9f8d5437f93641045deac317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a681b7dbd743b0a1396c559e94dc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebef5ed3c892401f8f3006ba5465fe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 125\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(dialogue: str) -> str:\n",
    "    return f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary: \"\n",
    "\n",
    "\n",
    "def causal_tokenize(batch):\n",
    "    input_ids_list, attn_list, labels_list = [], [], []\n",
    "\n",
    "    # Use RIGHT padding for training (more standard for causal LM training)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    for dialogue, summary in zip(batch[\"dialogue\"], batch[\"summary\"]):\n",
    "        prompt = build_prompt(dialogue)\n",
    "        # Add EOS token at the end of summary\n",
    "        full_text = prompt + summary + tokenizer.eos_token\n",
    "\n",
    "        tok_full = tokenizer(\n",
    "            full_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,  # Back to 512 for better context\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Tokenize prompt separately to find where summary starts\n",
    "        prompt_tok = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=False,  # Don't add BOS here since full_text already has it\n",
    "        )\n",
    "        prompt_len = len(prompt_tok[\"input_ids\"]) + 1  # +1 for BOS token\n",
    "\n",
    "        labels = tok_full[\"input_ids\"].copy()\n",
    "        # With RIGHT padding: mask prompt tokens at the start, and padding at the end\n",
    "        for i in range(len(labels)):\n",
    "            if i < prompt_len or tok_full[\"attention_mask\"][i] == 0:\n",
    "                labels[i] = -100\n",
    "\n",
    "        input_ids_list.append(tok_full[\"input_ids\"])\n",
    "        attn_list.append(tok_full[\"attention_mask\"])\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attn_list,\n",
    "        \"labels\": labels_list,\n",
    "    }\n",
    "\n",
    "\n",
    "causal_dataset = dataset.map(\n",
    "    causal_tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "# Use all available training data\n",
    "train_causal = causal_dataset[\"train\"]\n",
    "print(f\"Training samples: {len(train_causal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb72601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 9175040\n",
      "all model parameters: 1812638720\n",
      "percentage of trainable model parameters: 0.51%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Reduced from 32 to save memory\n",
    "    lora_alpha=32,  # 2x rank\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],  # Reduced target modules to save memory\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f08d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 10:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.883800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.776700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.568300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.216200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.6915067724883557, metrics={'train_runtime': 649.4924, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.246, 'total_flos': 5429611069440000.0, 'train_loss': 0.6915067724883557, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear GPU memory before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Enable gradient checkpointing with use_reentrant=False for PEFT compatibility\n",
    "peft_model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "output_dir = \"./training-output\"\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # Higher learning rate for LoRA\n",
    "    num_train_epochs=5,  # More epochs\n",
    "    logging_steps=5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=train_causal,\n",
    ")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21310ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegardé dans training-output\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"training-output\"\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(f\"Sauvegardé dans {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "827e790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt ---\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official...\n",
      "\n",
      "--- Réponse PEFT ---\n",
      " #Person1# asks Ms. Dawson to take dictation for an intra-office memorandum. #Person1# introduces the policy that all office communications are restricted to email and official memos and those who use Instant Messaging will face termination.\n",
      "--- Référence humaine ---\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Set back to left padding for generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "peft_llama = PeftModel.from_pretrained(\n",
    "    original_model,\n",
    "    SAVE_DIR,\n",
    "    is_trainable=False,\n",
    ")\n",
    "\n",
    "test_idx = 0\n",
    "test_dialogue = dataset[\"test\"][test_idx][\"dialogue\"]\n",
    "test_summary = dataset[\"test\"][test_idx][\"summary\"]\n",
    "\n",
    "test_prompt = build_prompt(test_dialogue)\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(peft_llama.device)\n",
    "\n",
    "outputs = peft_llama.generate(\n",
    "    **inputs,\n",
    "    generation_config=GenerationConfig(\n",
    "        max_new_tokens=128, temperature=0.7, do_sample=True\n",
    "    ),\n",
    ")\n",
    "# Decode only the NEW tokens (not the prompt)\n",
    "generated_text = tokenizer.decode(\n",
    "    outputs[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"--- Prompt ---\")\n",
    "print(test_prompt[:400] + \"...\\n\")\n",
    "print(\"--- Réponse PEFT ---\")\n",
    "print(generated_text)\n",
    "print(\"--- Référence humaine ---\")\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4894309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:20,  8.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8754232a-490b-4cfa-8227-5322f6b65be5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1# needs Ms. Dawson to take a dictation...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take dictation fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Person1# and Mike have a disagreement on how ...</td>\n",
       "      <td>#Person1# and #Person2# are doing an experimen...</td>\n",
       "      <td>#Person1# and #Person2# are doing a scene trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Person1# teaches #Person2# how to upgrade sof...</td>\n",
       "      <td>#Person1# suggests #Person2# to upgrade the sy...</td>\n",
       "      <td>#Person1# recommends #Person2# to upgrade the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person1# is crazy for Trump and voted for him...</td>\n",
       "      <td>#Person2# is supporting Trump, while #Person1#...</td>\n",
       "      <td>#Person1# thinks Trump isn't the right person,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person1# and #Person2# are talking about the ...</td>\n",
       "      <td>#Person1# and #Person2# talk about the heavy s...</td>\n",
       "      <td>#Person1# and #Person2# talk about the heavy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# tells David the plan for a tour and ...</td>\n",
       "      <td>#Person2# is making plans for a tour with #Per...</td>\n",
       "      <td>#Person2# is making plans for a tour with #Per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It's #Person1#'s first time to China and #Pers...</td>\n",
       "      <td>#Person1# is starving and #Person2# recommends...</td>\n",
       "      <td>#Person1# is starving and wants to try some re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person2# thinks #Person1#'s new suit is not w...</td>\n",
       "      <td>#Person2# thinks #Person1#'s new suit is not a...</td>\n",
       "      <td>#Person1# thinks #Person1#'s new suit is not a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person2# tells #Person1# about the relationsh...</td>\n",
       "      <td>#Person2# tells #Person1# about his family in ...</td>\n",
       "      <td>#Person2# tells #Person1# about Bill, his wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# asks #Person2# to scedule an emergen...</td>\n",
       "      <td>#Person1# and #Person2# need to call an emerge...</td>\n",
       "      <td>#Person1# and #Person2# need to call an emerge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8754232a-490b-4cfa-8227-5322f6b65be5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8754232a-490b-4cfa-8227-5322f6b65be5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8754232a-490b-4cfa-8227-5322f6b65be5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  #Person1# and Mike have a disagreement on how ...   \n",
       "2  #Person1# teaches #Person2# how to upgrade sof...   \n",
       "3  #Person1# is crazy for Trump and voted for him...   \n",
       "4  #Person1# and #Person2# are talking about the ...   \n",
       "5  #Person2# tells David the plan for a tour and ...   \n",
       "6  It's #Person1#'s first time to China and #Pers...   \n",
       "7  #Person2# thinks #Person1#'s new suit is not w...   \n",
       "8  #Person2# tells #Person1# about the relationsh...   \n",
       "9  #Person1# asks #Person2# to scedule an emergen...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  #Person1# needs Ms. Dawson to take a dictation...   \n",
       "1  #Person1# and #Person2# are doing an experimen...   \n",
       "2  #Person1# suggests #Person2# to upgrade the sy...   \n",
       "3  #Person2# is supporting Trump, while #Person1#...   \n",
       "4  #Person1# and #Person2# talk about the heavy s...   \n",
       "5  #Person2# is making plans for a tour with #Per...   \n",
       "6  #Person1# is starving and #Person2# recommends...   \n",
       "7  #Person2# thinks #Person1#'s new suit is not a...   \n",
       "8  #Person2# tells #Person1# about his family in ...   \n",
       "9  #Person1# and #Person2# need to call an emerge...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  #Person1# asks Ms. Dawson to take dictation fo...  \n",
       "1  #Person1# and #Person2# are doing a scene trai...  \n",
       "2  #Person1# recommends #Person2# to upgrade the ...  \n",
       "3  #Person1# thinks Trump isn't the right person,...  \n",
       "4  #Person1# and #Person2# talk about the heavy s...  \n",
       "5  #Person2# is making plans for a tour with #Per...  \n",
       "6  #Person1# is starving and wants to try some re...  \n",
       "7  #Person1# thinks #Person1#'s new suit is not a...  \n",
       "8  #Person2# tells #Person1# about Bill, his wife...  \n",
       "9  #Person1# and #Person2# need to call an emerge...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dialogues = dataset[\"test\"][0:10][\"dialogue\"]\n",
    "human_baseline_summaries = dataset[\"test\"][0:10][\"summary\"]\n",
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in tqdm(enumerate(dialogues)):\n",
    "    prompt = build_prompt(dialogue)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Generate with original model - use consistent generation config\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=150,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,  # Greedy for fair comparison\n",
    "    )\n",
    "\n",
    "    original_model_outputs = original_model.to(device).generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "    )\n",
    "    original_model_text_output = tokenizer.decode(\n",
    "        original_model_outputs[0][prompt_length:], skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    # Generate with PEFT model\n",
    "    peft_model_outputs = peft_llama.to(device).generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "    )\n",
    "    peft_model_text_output = tokenizer.decode(\n",
    "        peft_model_outputs[0][prompt_length:], skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(\n",
    "    zip(human_baseline_summaries, original_model_summaries, peft_model_summaries)\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    zipped_summaries,\n",
    "    columns=[\n",
    "        \"human_baseline_summaries\",\n",
    "        \"original_model_summaries\",\n",
    "        \"peft_model_summaries\",\n",
    "    ],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45422eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f11cf6b7c3b4d8bbc83bc4c5d503cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f687bbcb744498b186d08e4bd4ad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.22 seconds, 44.52 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aa13dac8564ba89ef067237fa8c243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe41f6141534f3895f18eb1bfca041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.21 seconds, 47.25 sentences/sec\n",
      "           original_model_summaries  peft_model_summaries\n",
      "precision                  0.914271              0.918608\n",
      "recall                     0.915814              0.914797\n",
      "f1                         0.914913              0.916576\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "def compute_bertscore(df, column_names):\n",
    "    \"\"\"Compute BERTScore for the specified columns in the DataFrame.\"\"\"\n",
    "    bert_scores = {}\n",
    "    references = df[\"human_baseline_summaries\"].tolist()\n",
    "\n",
    "    for col in column_names:\n",
    "        predictions = df[col].tolist()\n",
    "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "        bert_scores[col] = {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item(),\n",
    "        }\n",
    "    return bert_scores\n",
    "\n",
    "\n",
    "bert_scores = compute_bertscore(\n",
    "    df,\n",
    "    [\n",
    "        \"original_model_summaries\",\n",
    "        \"peft_model_summaries\",\n",
    "    ],\n",
    ")\n",
    "print(pd.DataFrame(bert_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36ea0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           difference_percentage\n",
      "precision               0.474316\n",
      "recall                 -0.111020\n",
      "f1                      0.181750\n"
     ]
    }
   ],
   "source": [
    "def calc_percentage_difference(scores, col_names):\n",
    "    \"\"\"Calculate the percentage difference in BERTScore between two columns.\"\"\"\n",
    "    percentage_differences = {}\n",
    "    col1, col2 = col_names\n",
    "    for metric in scores[col1].keys():\n",
    "        score1 = scores[col1][metric]\n",
    "        score2 = scores[col2][metric]\n",
    "        percentage_diff = (\n",
    "            ((score2 - score1) / score1) * 100 if score1 != 0 else float(\"inf\")\n",
    "        )\n",
    "        percentage_differences[metric] = percentage_diff\n",
    "    return percentage_differences\n",
    "\n",
    "\n",
    "diff_scores = calc_percentage_difference(\n",
    "    bert_scores,\n",
    "    [\n",
    "        \"original_model_summaries\",\n",
    "        \"peft_model_summaries\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\n",
    "    pd.DataFrame.from_dict(\n",
    "        diff_scores,\n",
    "        orient=\"index\",\n",
    "        columns=[\n",
    "            \"difference_percentage\",\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
